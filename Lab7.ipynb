{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 - Evaluation and Metrics\n",
    "\n",
    "In a previous lab, we looked at how to train and test a POS-tagger. We used the evaluation concept to track how our POS-tagger was improving as we added more models to it. In this lab we will be looking at how we evaluate our models and what metrics we can use to see how our model is doing.\n",
    "\n",
    "We will use [chapter 6](http://www.nltk.org/book/ch06.html) of the nltk book as our reference for this lab and we will work through one of the examples while focusing on the evaluation of our model and the various metrics we use for this.\n",
    "\n",
    "In this lab, we will train a model for detecting the gender of names. First we need to identify what features we want to use to train our model. For our model, we will start of with using one feature: the last letter of the name. Let's write a feature extractor in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractor(name):\n",
    "    # name[-1] will select the last letter of the name\n",
    "    return {'suffix1': name[-1:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our feature extractor we define and sort our data. We will be using the nltk 'names' corpus which consists of two files: males.txt, females.txt (no, gender is not binary; it's just the way it's encoded here).\n",
    "\n",
    "We will extract the names and store them in a dictionary format with the name as key and the gender as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the corpus and the random module\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "\n",
    "# construct the dataset\n",
    "labeledNames = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# shuffle the data set\n",
    "random.shuffle(labeledNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how we will split up our dataset for training and evaluation. We will split up the dataset into a development set and a test set. As the names imply, the development set will be used for training and developing our model. Once we have a working model we can test it on our test set and get our evaluation metrics. \n",
    "\n",
    "The development set itself is further divided into a **training set** and a **development test set**. The training set is used to train our initial model and the development test set will be used to test our initial model and tweak it before testing it on our final test set. The reason for a separate development test set is that once we test our model on this test set and modify our model, this test set that we used can no longer give us accurate metrics for accuracy, since we used it to tweak the model. (Note that some people refer to this training + development test set as the **training set**. We are using the term **develoment** for the overall combination.)\n",
    "\n",
    "So, in the next code block we generate a feature set from our data and split this up into the relevant sub sets. A 25/75 split of the data into development data and test data is advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we split the names up into three sets. We know there are 2,000 names\n",
    "trainingNames = labeledNames[1500:]\n",
    "devtestNames = labeledNames[500:1500]\n",
    "testNames = labeledNames[:500]\n",
    "\n",
    "# next we extract the features from each set\n",
    "trainingSet = [(featureExtractor(n), gender) for (n, gender) in trainingNames]\n",
    "devtestSet = [(featureExtractor(n), gender) for (n, gender) in devtestNames]\n",
    "testSet = [(featureExtractor(n), gender) for (n, gender) in testNames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our various datasets, we can start training our model using the training set. We will be using a \"naive Bayes\" classifier which you can read more about in part 5 of the chapter reading linked above. We will then test it on our devtestSet to see where we can improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(trainingSet)\n",
    "print(nltk.classify.accuracy(classifier, devtestSet))\n",
    "\n",
    "# We can check what the most informative features are in our model\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, names ending in 'a' are predominantly female according to our classifier and names ending in 'k' are mostly male. To improve our model, we will generate a list of names that our classifier gets wrong using the devtestSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for (name, tag) in devtestNames:\n",
    "    guess = classifier.classify(featureExtractor(name))\n",
    "    if guess != tag:\n",
    "        print(\"correct=%s guess=%s name=%s\" % (tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our classifier only looks at the the last letter of each name. From this list, however, we see that sometimes the last two letters are a better indicator of gender. This is because names ending in 'yn' or 'en' are mostly female, even though most names ending in 'n' are male. This tells us that we should add another feature to our model to improve it. This second feature will be the second to last letter of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractor2(name):\n",
    "    # suffix1 returns the last letter of the name and suffix2 returns the last two letters\n",
    "    return {'suffix1': name[-1:], 'suffix2': name[-2:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created a second feature extractor, let's re-train our model before we can finally test it on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet2 = [(featureExtractor2(n), gender) for (n, gender) in trainingNames]\n",
    "devtestSet2 = [(featureExtractor2(n), gender) for (n, gender) in devtestNames]\n",
    "testSet2 = [(featureExtractor2(n), gender) for (n, gender) in testNames]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainingSet2)\n",
    "print(nltk.classify.accuracy(classifier, devtestSet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model improved by about 2% by adding one extra feature. But we need to remember that we are only testing on our devtestSet here. This is the set that we investigated and optimized for. \n",
    "\n",
    "In order to really know how our model is doing, we need to test it on data that it has never seen before. This is why we have a testSet which we do not use until our model is finalized. Let's run this final model on our testSet and see the actual expected accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, testSet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model got around 76%-81% of tags correct depending on how the data was shuffled. This is fairly good since our model has never seen this data. \n",
    "\n",
    "Since this lab is about evaluation metrics let's look at some other ways to see how our model is doing. Accuracy is a very basic evaluation metric and doesn't always give us very good information about our model.\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "In binary classification tasks, ones where the answer is either correct or false, precision and recall can tell us a lot about our model. In order to calculate these two metrics we first need to split our data into four categories:\n",
    "\n",
    "- True Positives(TP): Items that were correctly labeled true\n",
    "- True Negatives(TN): Items that were correctly labeled false\n",
    "- False Positives(FP): Items that were incorrectly labeled true\n",
    "- False Negatives(FN): Items that were incorrectly labeled false\n",
    "\n",
    "Once we have the numbers for the above categories, we can calculate the precision and recall of our model. Precision will tell us how many of the retieved instances are correct or, in other words, how much we can trust the output. Recall, on the other hand, will tell us how many of the relevant instances are found. For a good read on precision and recall look at this [5 minute read](https://medium.com/@yashwant140393/the-3-pillars-of-binary-classification-accuracy-precision-recall-d2da3d09f664#:~:text=PRECISION%3A,total%20number%20of%20positive%20calls.)\n",
    "\n",
    "Now, to calculate the precision of a model we divide the true positives by true positives plus false positives: TP/TP+FP\n",
    "\n",
    "To calculate the recall of a model we divide the true positives by true positives plus false negatives: TP/TP+FN\n",
    "\n",
    "Let's now find the precision and recall for tagging male and female names. Since precision and recall are for binary classification taks, we will need to formulate our question as follows: \n",
    "\n",
    "1. What is the precision and recall for male names?\n",
    "2. What is the precision and recall for female names?\n",
    "\n",
    "I will show you how to do 1 and then you will follow up by doing 2. \n",
    "\n",
    "Let's calculate the following:\n",
    "- TP: male names labeled as male\n",
    "- TN: female names labeled as female\n",
    "- FP: female names labeled as male\n",
    "- FN: male names labeled as female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our variables to 0\n",
    "TP = TN = FN = FP = 0\n",
    "\n",
    "# loop over all the names in our test set\n",
    "for (name, tag) in devtestNames:\n",
    "    # guess the label according to our classifier\n",
    "    guess = classifier.classify(featureExtractor(name))\n",
    "    \n",
    "    # add one to the count depending on the category\n",
    "    # if the correct tag is male and the model guesses correctly it's a TP\n",
    "    if tag == \"male\" and guess == tag:\n",
    "        TP += 1\n",
    "    # if the correct tag is female and the model guesses correctly it's a TN\n",
    "    if tag == \"female\" and guess == tag:\n",
    "        TN += 1\n",
    "    # if the correct tag is male and the model does not guess correct it's a FN\n",
    "    if tag == \"male\" and guess != tag:\n",
    "        FN += 1\n",
    "    # if the correct tag is female and the model does not guess correct it's a FP\n",
    "    if tag == \"female\" and guess != tag:\n",
    "        FP += 1\n",
    "        \n",
    "print(\"TP = \", TP)\n",
    "print(\"TN = \", TN)\n",
    "print(\"FN = \", FN)\n",
    "print(\"FP = \", FP)\n",
    "\n",
    "# calculate precision and recall\n",
    "print(\"Precision = \", TP/(TP+FP))\n",
    "print(\"Recall = \", TP/(TP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model is fairly good at precision. When the model tells us a name is male, we can be ~70% sure that this label is correct. The recall, however, is a little worse. This means that our model isn't very good at finding male names and about 40% of all male names are missed.\n",
    "\n",
    "Another metric that you will often find in research is the F-score or F-measure. This metric combines precision and recall into one single score. To find the F-score of a model you use the following formula: (2*Precison*Recall)/(Precision+Recall)\n",
    "\n",
    "Calculate the precision, recall and F-score for identifying female names in the next code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our variables to 0\n",
    "TP = TN = FN = FP = 0\n",
    "\n",
    "# loop over all the names in our test set\n",
    "for (name, tag) in devtestNames:\n",
    "    # guess the label according to our classifier\n",
    "    guess = classifier.classify(featureExtractor(name))\n",
    "    \n",
    "    # add the correct if statements here to count each occurence\n",
    "    \n",
    "# print the counts for each category as well as precison, recall, and F-score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
